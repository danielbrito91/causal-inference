{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "url_email_obs = \"https://raw.githubusercontent.com/matheusfacure/causal-inference-in-python-code/main/causal-inference-in-python/data/email_obs_data.csv\"\n",
    "url_email_rnd = \"https://raw.githubusercontent.com/matheusfacure/causal-inference-in-python-code/main/causal-inference-in-python/data/email_rnd_data.csv\"\n",
    "\n",
    "data_biased = pd.read_csv(url_email_obs)\n",
    "data_rnd = pd.read_csv(url_email_rnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"next_mnth_pv\"\n",
    "T = \"mkt_email\"\n",
    "X = list(data_rnd.drop(columns=[y, T]).columns)\n",
    "\n",
    "train, test = data_biased, data_rnd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "np.random.seed(123)\n",
    "m0 = LGBMRegressor()\n",
    "m1 = LGBMRegressor()\n",
    "\n",
    "m0.fit(train.query(f\"{T}==0\")[X], train.query(f\"{T}==0\")[y])\n",
    "m1.fit(train.query(f\"{T}==1\")[X], train.query(f\"{T}==1\")[y])\n",
    "\n",
    "t_learner_cate_test = test.assign(cate=m1.predict(test[X]) - m0.predict(test[X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from toolz import curry\n",
    "\n",
    "@curry\n",
    "def effect(data, y, t):\n",
    "    return (np.sum((data[t] - data[t].mean()) * data[y]) / sum( (data[t] - data[t].mean())**2 ))\n",
    "\n",
    "def cumulative_gain_curve(df, prediction, y, t, ascending=False, normalize=False, steps=100):\n",
    "    effect_fn = effect(t = t, y = y)\n",
    "    normalizer = effect_fn(df) if normalize else 0\n",
    "\n",
    "    size = len(df)\n",
    "    ordered_df = (df\n",
    "                  .sort_values(prediction, ascending=ascending)\n",
    "                  .reset_index(drop=True))\n",
    "    \n",
    "    steps = np.linspace(size / steps, size, steps).round(0)\n",
    "    effects = [(effect_fn(ordered_df.query(f\"index <= {row}\")) - normalizer) * (row / size) for row in steps]\n",
    "\n",
    "    return np.array([0] + effects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10532.16173183944"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trapz(cumulative_gain_curve(t_learner_cate_test, \"cate\", y, T, normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# Propensity score model\n",
    "ps_model = LogisticRegression(penalty=\"none\")\n",
    "ps_model.fit(train[X], train[T])\n",
    "\n",
    "# First stage models\n",
    "train_t0 = train.query(f\"{T}==0\")\n",
    "train_t1 = train.query(f\"{T}==1\")\n",
    "\n",
    "m0 = LGBMRegressor()\n",
    "m1 = LGBMRegressor()\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "m0.fit(train_t0[X], train_t0[y], sample_weight=1 / ps_model.predict_proba(train_t0[X])[:, 0])\n",
    "m1.fit(train_t1[X], train_t1[y], sample_weight=1 / ps_model.predict_proba(train_t1[X])[:, 1])\n",
    "\n",
    "# Second stage\n",
    "tau_hat_0 = m1.predict(train_t0[X]) - train_t0[y]\n",
    "tau_hat_1 = m0.predict(train_t1[X]) - train_t1[y]\n",
    "\n",
    "m_tau_0 = LGBMRegressor()\n",
    "m_tau_1 = LGBMRegressor()\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "m_tau_0.fit(train_t0[X], tau_hat_0)\n",
    "m_tau_1.fit(train_t1[X], tau_hat_1)\n",
    "\n",
    "# estimate the Cate\n",
    "ps_test = ps_model.predict_proba(test[X])[:, 1]\n",
    "x_cate_test = test.assign(\n",
    "    cate = (ps_test * m_tau_0.predict(test[X]) + (1 - ps_test)*m_tau_1.predict(test[X]))\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3083.5822393048334"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.trapz(cumulative_gain_curve(x_cate_test, \"cate\", y, T, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
